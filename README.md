# üß† AI Document Chatbot & Theme Identifier ‚Äì Wasserstoff Internship Task

This project was developed as part of the AI Internship Task at **Wasserstoff Innovation and Learning Labs**.

The application allows users to upload various document formats (including PDFs, scanned images, and text files), ask questions based on the content of these documents, and receive context-aware answers generated by a Large Language Model. Additionally, it can extract and display common themes present across the uploaded documents. The entire workflow is powered by Generative AI tools, semantic vector search, and an interactive Streamlit interface.

**üîó Live Application URL:** [https://wasserstoff-ai-chatbot.streamlit.app/](https://wasserstoff-ai-chatbot.streamlit.app/)

---

## üöÄ Features

* üìÑ **Versatile Document Upload:** Upload PDFs, scanned images (PNG, JPG, JPEG), and TXT files.
* üîç **Intelligent Text Extraction:** Utilizes OCR (via Pytesseract) for images and advanced parsing (via PdfPlumber) for PDFs to accurately extract textual content.
* üß© **Smart Text Chunking & Embedding:** Text is intelligently segmented into manageable chunks and converted into vector embeddings using Google Gemini Embeddings.
* üîé **Semantic Vector Search:** Leverages ChromaDB to store and search for the most semantically relevant document chunks corresponding to user queries.
* ü§ñ **AI-Powered Question Answering:** Employs the Google Gemini API (via LangChain) to generate answers based on the retrieved document context, complete with source document citations.
* üß† **Automated Theme Extraction:** Identifies and extracts key themes from the processed documents using the Google Gemini API, presented in a clear, tabular format.
* üí° **Interactive User Interface:** A user-friendly and responsive web interface built entirely with Streamlit.

---

## üõ†Ô∏è Tech Stack

| Component             | Tool / Library                                       |
| --------------------- | ---------------------------------------------------- |
| Programming Language  | Python 3.9+                                          |
| UI Framework          | Streamlit                                            |
| LLM Orchestration     | LangChain                                            |
| OCR Engine            | Tesseract (via `pytesseract`)                        |
| PDF Parser            | `pdfplumber`                                         |
| Vector Embeddings     | Google Gemini Embeddings (`langchain-google-genai`)  |
| Vector Store & Search | ChromaDB                                             |
| LLM for QA & Themes | Google Gemini API (e.g., `gemini-1.5-flash-latest`)  |
| Data Handling         | Pandas                                               |
| SQLite Fix            | `pysqlite3-binary`                                   |

---

## üß™ How It Works (Application Workflow)

1.  **Document Upload:** The user uploads one or more documents (PDF, image, TXT) through the Streamlit interface.
2.  **Text Extraction:** The backend processes these files:
    * OCR (Pytesseract) is used for image files.
    * PdfPlumber is used for PDF text extraction.
    * Plain text is read directly from TXT files.
    * The extracted text is cleaned and saved.
3.  **Knowledge Base Initialization (User Triggered):**
    * The extracted texts are loaded.
    * Texts are split into smaller, overlapping chunks using LangChain's text splitters.
    * These chunks are converted into dense vector embeddings using Google Gemini Embeddings.
    * The embeddings and corresponding text chunks are stored and indexed in a ChromaDB vector store.
4.  **Question Answering:**
    * The user types a question in the Streamlit interface.
    * The question is embedded using the same Gemini embedding model.
    * A semantic search is performed against the ChromaDB vector store to retrieve the most relevant document chunks.
    * These relevant chunks (context) and the original question are passed to the Google Gemini LLM.
    * The LLM generates an answer based on the provided context.
    * The answer and the names of the source documents are displayed to the user.
5.  **Theme Extraction:**
    * The user clicks the "Extract Common Themes" button.
    * A significant portion of the text from the processed documents (or representative chunks from the vector store) is compiled.
    * This compiled text is sent to the Google Gemini LLM with a prompt asking it to identify common themes, their descriptions, and supporting documents in a structured JSON format.
    * The JSON response is parsed and displayed in a table in the Streamlit UI.

---

## ‚ñ∂Ô∏è How to Use This Application

The application is deployed and accessible via the live URL provided above.

1.  **Navigate to the "Upload & Process Documents" section:**
    * Use the file uploader to select your document(s).
    * Click the "Process Uploaded Files" button to extract text.
2.  **Go to the Sidebar and Initialize:**
    * Click the "Initialize/Re-Initialize Knowledge Base" button. Wait for the confirmation message. This step is crucial and may take a moment.
3.  **Interact:**
    * **Ask Questions:** In the "Ask Questions" section, type your query and get AI-generated answers with sources.
    * **Extract Themes:** In the "Extract Themes" section, click the button to get a tabular summary of common themes from your documents.

---

## üì¨ Submission By

* **Name:** Aviral Meharishi
* **Position:** AI Intern (Task Applicant)
* **Company:** Wasserstoff Innovation & Learning Labs
